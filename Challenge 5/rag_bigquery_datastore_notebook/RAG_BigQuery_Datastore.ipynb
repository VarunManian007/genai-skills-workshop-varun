{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install required dependency and SDK"
      ],
      "metadata": {
        "id": "_hb_eDTbTomQ"
      },
      "id": "_hb_eDTbTomQ"
    },
    {
      "cell_type": "code",
      "id": "sZnuZWRdswnLyMdS2pkh2IPf",
      "metadata": {
        "tags": [],
        "id": "sZnuZWRdswnLyMdS2pkh2IPf"
      },
      "source": [
        "!pip install --quiet google-cloud-bigquery google-cloud-aiplatform pandas"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the environment for the notebook\n",
        "1. Create a source connection and grant IAM permissions (Steps from https://www.cloudskillsboost.google/course_templates/1210/labs/529948)\n",
        "\n",
        "2. Generate embeddings\n",
        "\n",
        "\n",
        "```\n",
        "CREATE OR REPLACE MODEL `Alaska_Dataset.Embeddings`\n",
        "REMOTE WITH CONNECTION `us.embedding_conn`\n",
        "OPTIONS (ENDPOINT = 'text-embedding-005');\n",
        "```\n",
        "\n",
        "3. Created a Bigquery client so that I can directly use the Bigquery SQL syntax in the code. (ref: https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client#google_cloud_bigquery_client_Client_query)"
      ],
      "metadata": {
        "id": "2W07SO9BTwlr"
      },
      "id": "2W07SO9BTwlr"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from vertexai.preview.language_models import ChatModel\n",
        "import vertexai\n",
        "import pandas as pd\n",
        "\n",
        "PROJECT_ID = \"qwiklabs-gcp-04-d9bb68112d04\"\n",
        "LOCATION = \"global\"\n",
        "BQ_DATASET = \"Alaska_Dataset\"\n",
        "TABLE_RAW = \"Alaska_Dataset_data\"\n",
        "TABLE_EMBEDDED = \"Alaska_Dataset_embedded\"\n",
        "EMBED_MODEL = \"Alaska_Dataset_embeddings\"\n",
        "TABLE_ID = f\"{PROJECT_ID}.{BQ_DATASET}.{TABLE_EMBEDDED}\"\n",
        "RAW_TABLE_ID = f\"{PROJECT_ID}.{BQ_DATASET}.{TABLE_RAW}\"\n",
        "EMBED_MODEL_ID = f\"{PROJECT_ID}.{BQ_DATASET}.{EMBED_MODEL}\"\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "print(\"Environment set up.\", TABLE_ID, EMBED_MODEL_ID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp-ND_xhTDgc",
        "outputId": "d3ddb501-6dcd-4c43-8d21-bdc947dc768c"
      },
      "id": "sp-ND_xhTDgc",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment set up. qwiklabs-gcp-04-d9bb68112d04.Alaska_Dataset.Alaska_Dataset_embedded qwiklabs-gcp-04-d9bb68112d04.Alaska_Dataset.Alaska_Dataset_embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSV upload to the BiqQuery table\n",
        "\n",
        "1. Upload data from gs://labs.roitraining.com/alaska-dept-of-snow/alaska-dept-of-snow-faqs.csv to table (Alaska_Dataset_data)"
      ],
      "metadata": {
        "id": "4stvrA11UCSz"
      },
      "id": "4stvrA11UCSz"
    },
    {
      "cell_type": "code",
      "source": [
        "uri = \"gs://labs.roitraining.com/alaska-dept-of-snow/alaska-dept-of-snow-faqs.csv\"\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,\n",
        "    autodetect=True,\n",
        "    write_disposition=\"WRITE_TRUNCATE\"\n",
        ")\n",
        "load_job = bq_client.load_table_from_uri(uri, RAW_TABLE_ID, job_config=job_config)\n",
        "load_job.result()\n",
        "print(f\"FAQ CSV loaded into BigQuery table: {RAW_TABLE_ID}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHOeD8ZXTTeK",
        "outputId": "77d35340-e798-45df-8c8f-344e9f78a09a"
      },
      "id": "GHOeD8ZXTTeK",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAQ CSV loaded into BigQuery table: qwiklabs-gcp-04-d9bb68112d04.Alaska_Dataset.Alaska_Dataset_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Embedding Models\n",
        " 1. Creates a Remote Model in BigQuery ML\n",
        "This SQL command registers a remote Vertex AI model (text-embedding-005) as a BigQuery ML model so you can use it in ML.GENERATE_EMBEDDING() queries.\n",
        "\n",
        "2. Uses a Preconfigured Connection\n",
        "The REMOTE WITH CONNECTION clause refers to a BigQuery connection (us.embedding_conn) that links BigQuery to Vertex AI's embedding endpoint.\n",
        "\n",
        "3. Executes SQL via Python Client\n",
        "The Python code executes the SQL command using BigQuery's Python client and waits for it to complete using .result()."
      ],
      "metadata": {
        "id": "EBUGyuBwUS1h"
      },
      "id": "EBUGyuBwUS1h"
    },
    {
      "cell_type": "code",
      "source": [
        "create_model_sql = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{EMBED_MODEL_ID}`\n",
        "REMOTE WITH CONNECTION `us.embedding_conn`\n",
        "OPTIONS (ENDPOINT = 'text-embedding-005');\n",
        "\"\"\"\n",
        "bq_client.query(create_model_sql).result()\n",
        "print(\"Remote embedding model created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkXlVBNMTuVT",
        "outputId": "6b0cb643-527f-4b01-d13d-180ef1184135"
      },
      "id": "AkXlVBNMTuVT",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remote embedding model created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Embeddings Using BigQuery ML\n",
        "\n",
        "1. Generate Embeddings Using BigQuery ML produces an embedding vector using the model referenced.\n",
        "2. Store Results in a New Table.\n",
        "3. Execute via BigQuery Python Client.\n",
        "\n",
        "# Note:\n",
        "1. Since the loading of the csv we dont know what are the columns **string_field_0** and **string_field_1** is generated automatically.\n",
        "2. Here\n",
        "  1. **string_field_0** -  Question\n",
        "  2. **string_field_1** - Answer"
      ],
      "metadata": {
        "id": "A0wgxubIUOJV"
      },
      "id": "A0wgxubIUOJV"
    },
    {
      "cell_type": "code",
      "source": [
        "generate_embed_sql = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{TABLE_ID}` AS\n",
        "SELECT *, ml_generate_embedding_result AS embedding\n",
        "FROM ML.GENERATE_EMBEDDING(\n",
        "  MODEL `{EMBED_MODEL_ID}`,\n",
        "  (\n",
        "    SELECT CONCAT(string_field_0, ' ', string_field_1) AS content,\n",
        "           string_field_0 AS question,\n",
        "           string_field_1 AS answer\n",
        "    FROM `{RAW_TABLE_ID}`\n",
        "  )\n",
        ");\n",
        "\"\"\"\n",
        "bq_client.query(generate_embed_sql).result()\n",
        "print(\"Embeddings generated and stored.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pLWFWdoT1PC",
        "outputId": "fdf879e8-dea5-45b8-8d44-1696653c1649"
      },
      "id": "6pLWFWdoT1PC",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings generated and stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Vector Search Results from BigQuery Table is computed in the python app for the chat bot."
      ],
      "metadata": {
        "id": "jYWemp6eUvpv"
      },
      "id": "jYWemp6eUvpv"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "student-01-35678b206598 (Jun 17, 2025, 10:14:00â€¯AM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}