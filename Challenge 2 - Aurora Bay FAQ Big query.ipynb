{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install required dependency and SDK"
      ],
      "metadata": {
        "id": "ZVELBgkE1VRK"
      },
      "id": "ZVELBgkE1VRK"
    },
    {
      "cell_type": "code",
      "id": "Ttn8R5zZBdrBJ8yXdZJ6pxA4",
      "metadata": {
        "tags": [],
        "id": "Ttn8R5zZBdrBJ8yXdZJ6pxA4"
      },
      "source": [
        "!pip install --quiet google-cloud-bigquery google-cloud-aiplatform pandas"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the environment for the notebook\n",
        "1. Create a source connection and grant IAM permissions (Steps from https://www.cloudskillsboost.google/course_templates/1210/labs/529948)\n",
        "\n",
        "2. Generate embeddings\n",
        "\n",
        "\n",
        "```\n",
        "CREATE OR REPLACE MODEL `aurora_bay_faqs_conn.Embeddings`\n",
        "REMOTE WITH CONNECTION `us.embedding_conn`\n",
        "OPTIONS (ENDPOINT = 'text-embedding-005');\n",
        "```\n",
        "\n",
        "3. Created a Bigquery client so that I can directly use the Bigquery SQL syntax in the code. (ref: https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client#google_cloud_bigquery_client_Client_query)"
      ],
      "metadata": {
        "id": "IJRL0mqk1Ud7"
      },
      "id": "IJRL0mqk1Ud7"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from vertexai.preview.language_models import ChatModel\n",
        "import vertexai\n",
        "import pandas as pd\n",
        "\n",
        "PROJECT_ID = \"qwiklabs-gcp-04-d9bb68112d04\"\n",
        "LOCATION = \"global\"\n",
        "BQ_DATASET = \"aurora_bay_faqs_conn\"\n",
        "TABLE_RAW = \"faq_data\"\n",
        "TABLE_EMBEDDED = \"faq_data_embedded\"\n",
        "EMBED_MODEL = \"faq_embeddings\"\n",
        "TABLE_ID = f\"{PROJECT_ID}.{BQ_DATASET}.{TABLE_EMBEDDED}\"\n",
        "RAW_TABLE_ID = f\"{PROJECT_ID}.{BQ_DATASET}.{TABLE_RAW}\"\n",
        "EMBED_MODEL_ID = f\"{PROJECT_ID}.{BQ_DATASET}.{EMBED_MODEL}\"\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "print(\"Environment set up.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6gbRK3na1tl",
        "outputId": "cd0b2db0-9d75-4b0b-a7ba-260235a09ede"
      },
      "id": "-6gbRK3na1tl",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment set up.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSV upload to the BiqQuery table\n",
        "\n",
        "1. Upload data from gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv table (faq_data)"
      ],
      "metadata": {
        "id": "i7qd3Cuo2xwt"
      },
      "id": "i7qd3Cuo2xwt"
    },
    {
      "cell_type": "code",
      "source": [
        "uri = \"gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv\"\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,\n",
        "    autodetect=True,\n",
        "    write_disposition=\"WRITE_TRUNCATE\"\n",
        ")\n",
        "load_job = bq_client.load_table_from_uri(uri, RAW_TABLE_ID, job_config=job_config)\n",
        "load_job.result()\n",
        "print(f\"FAQ CSV loaded into BigQuery table: {RAW_TABLE_ID}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlCSQywxb5i1",
        "outputId": "be2f66f7-2e61-402e-9ca3-e6d8359e0624"
      },
      "id": "rlCSQywxb5i1",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAQ CSV loaded into BigQuery table: qwiklabs-gcp-04-d9bb68112d04.aurora_bay_faqs_conn.faq_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Create Embedding Models\n",
        " 1. Creates a Remote Model in BigQuery ML\n",
        "This SQL command registers a remote Vertex AI model (text-embedding-005) as a BigQuery ML model so you can use it in ML.GENERATE_EMBEDDING() queries.\n",
        "\n",
        "2. Uses a Preconfigured Connection\n",
        "The REMOTE WITH CONNECTION clause refers to a BigQuery connection (us.embedding_conn) that links BigQuery to Vertex AI's embedding endpoint.\n",
        "\n",
        "3. Executes SQL via Python Client\n",
        "The Python code executes the SQL command using BigQuery's Python client and waits for it to complete using .result()."
      ],
      "metadata": {
        "id": "Vlvh6i9a3PdT"
      },
      "id": "Vlvh6i9a3PdT"
    },
    {
      "cell_type": "code",
      "source": [
        "create_model_sql = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{EMBED_MODEL_ID}`\n",
        "REMOTE WITH CONNECTION `us.embedding_conn`\n",
        "OPTIONS (ENDPOINT = 'text-embedding-005');\n",
        "\"\"\"\n",
        "bq_client.query(create_model_sql).result()\n",
        "print(\"Remote embedding model created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qK4SWGkicmk",
        "outputId": "f60b5c84-06af-4ed7-b6a2-a188364743ae"
      },
      "id": "4qK4SWGkicmk",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remote embedding model created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Embeddings Using BigQuery ML\n",
        "\n",
        "1. Generate Embeddings Using BigQuery ML produces an embedding vector using the model referenced.\n",
        "2. Store Results in a New Table.\n",
        "3. Execute via BigQuery Python Client.\n",
        "\n",
        "# Note:\n",
        "1. Since the loading of the csv we dont know what are the columns **string_field_0** and **string_field_1** is generated automatically.\n",
        "2. Here\n",
        "  1. **string_field_0** -  Question\n",
        "  2. **string_field_1** - Answer"
      ],
      "metadata": {
        "id": "wH6kKref3t90"
      },
      "id": "wH6kKref3t90"
    },
    {
      "cell_type": "code",
      "source": [
        "generate_embed_sql = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{TABLE_ID}` AS\n",
        "SELECT *, ml_generate_embedding_result AS embedding\n",
        "FROM ML.GENERATE_EMBEDDING(\n",
        "  MODEL `{EMBED_MODEL_ID}`,\n",
        "  (\n",
        "    SELECT CONCAT(string_field_0, ' ', string_field_1) AS content,\n",
        "           string_field_0 AS question,\n",
        "           string_field_1 AS answer\n",
        "    FROM `{RAW_TABLE_ID}`\n",
        "  )\n",
        ");\n",
        "\"\"\"\n",
        "bq_client.query(generate_embed_sql).result()\n",
        "print(\"Embeddings generated and stored.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXVPVhxbjLI2",
        "outputId": "0e724a4c-8222-4b61-d250-8a9941da06cb"
      },
      "id": "HXVPVhxbjLI2",
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings generated and stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Vector Search Results from BigQuery Table\n",
        "\n",
        "1. This function queries the BigQuery table that stores VECTOR_SEARCH results.\n",
        "2. It filters results by the current user question and returns top 3 most relevant rows"
      ],
      "metadata": {
        "id": "G33DAvwo4oFb"
      },
      "id": "G33DAvwo4oFb"
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_faq_results(user_question):\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "      query.query,\n",
        "      result.base.question,\n",
        "      result.base.answer,\n",
        "      result.distance\n",
        "    FROM VECTOR_SEARCH(\n",
        "      TABLE `{TABLE_ID}`,\n",
        "      'embedding',\n",
        "      (\n",
        "        SELECT\n",
        "          ml_generate_embedding_result AS embedding,\n",
        "          '{user_question}' AS query\n",
        "        FROM ML.GENERATE_EMBEDDING(\n",
        "          MODEL `{EMBED_MODEL_ID}`,\n",
        "          (SELECT '{user_question}' AS content)\n",
        "        )\n",
        "      ),\n",
        "      top_k => 3,\n",
        "      options => '{{\"fraction_lists_to_search\": 1.0}}'\n",
        "    ) AS result\n",
        "    \"\"\"\n",
        "    return bq_client.query(query).to_dataframe()"
      ],
      "metadata": {
        "id": "c0DDNTGAktr8"
      },
      "id": "c0DDNTGAktr8",
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini Chat Completion\n",
        "\n",
        "1. This function uses the Gemini generative model to generate a chatbot response\n",
        "2. It builds a prompt from top FAQ results retrieved from BigQuery"
      ],
      "metadata": {
        "id": "stwrtVMm48ec"
      },
      "id": "stwrtVMm48ec"
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview.generative_models import GenerativeModel\n",
        "chat_model = GenerativeModel(\"gemini-2.0-flash-001\")\n",
        "\n",
        "def generate_bot_response(user_input):\n",
        "    results = fetch_faq_results(user_input)\n",
        "    context = \"\\n\\n\".join([f\"Q: {row['question']}\\nA: {row['answer']}\" for _, row in results.iterrows()])\n",
        "    prompt = f\"You are a helpful assistant for the town of Aurora Bay. Use the following FAQ context to answer:\\n\\n{context}\\n\\nUser: {user_input}\"\n",
        "    response = chat_model.generate_content(prompt)\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "VmTyts9CkyqZ"
      },
      "id": "VmTyts9CkyqZ",
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Chatbot\n",
        "Send a sample question to the chatbot and display the response"
      ],
      "metadata": {
        "id": "-96ZTK8a5Eku"
      },
      "id": "-96ZTK8a5Eku"
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    question = input(\"User: \")\n",
        "    if question.strip().lower() in [\"exit\", \"no\", \"quit\"]:\n",
        "        print(\"\\nSession ended.\")\n",
        "        break\n",
        "    print(\"\\nAssistant:\", generate_bot_response(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJtOH5QGk2hP",
        "outputId": "37e15422-5a24-4950-bb23-f7c906a9c1d3"
      },
      "id": "rJtOH5QGk2hP",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What are the policies in aurora bay?\n",
            "\n",
            "Assistant: Could you please clarify what kind of policies you're interested in? Are you looking for fishing regulations, temperature information, recreational activities policies, or something else entirely? The more specific you are, the better I can assist you.\n",
            "\n",
            "User: what are the fishing regulations in aurora bay?\n",
            "\n",
            "Assistant: You can find the fishing regulations published by the Aurora Bay Fish & Game Department. They are located at 45 Coastal Drive. You can also find a digital copy of the guidelines on the town's official website.\n",
            "\n",
            "User: what is the temperature range in aurora bay?\n",
            "\n",
            "Assistant: Winters average between 10°F to 25°F, while summers are milder, around 50°F to 65°F. Keep in mind that temperatures can vary with coastal weather patterns.\n",
            "\n",
            "User: exit\n",
            "\n",
            "Session ended.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "student-01-35678b206598 (Jun 16, 2025, 11:53:21 AM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}